{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90eeb878",
   "metadata": {},
   "source": [
    "# <font color=\"#3A40A2\">üìò 4. Reducci√≥n de dimensionalidad</font>\n",
    "\n",
    "**Materia: Ciencia de Datos aplicada a los Negocios - Universidad de San Andr√©s**\n",
    "\n",
    "**Autor: [Lucas BALDEZZARI](https://www.linkedin.com/in/lucasbaldezzari/)**\n",
    "\n",
    "**2025**\n",
    "\n",
    "> Este material es para fines educativos y no debe ser utilizado para fines comerciales. El contenido pertenece a la *Universidad de San Andr√©s* y no debe ser reproducido sin el permiso expl√≠cito de la instituci√≥n y del autor de este repositorio quien es [LUCAS BALDEZZARI](https://www.linkedin.com/feed/).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49751357",
   "metadata": {},
   "source": [
    "## <font color=\"#004eb3\">Temas de la Colab</font>\n",
    "\n",
    "Para esta clase, los temas que veremos son:\n",
    "\n",
    "- Aplicaciones de PCA y LDA para entender los fundamentos de funcionamiento cada algoritmo y la importancia de la reducci√≥n de dimensionalidad.\n",
    "- Entender de manera intuitiva, sin necesidad de profundizar en la matem√°tica detr√°s de cada t√©cnica.\n",
    "- Entender las diferencias clave entre PCA y LDA, y c√≥mo\n",
    "\n",
    "Es importante prestar atenci√≥n a los siguientes √≠conos o emojis que aparezcan a lo largo de la Colab.\n",
    "\n",
    "- üìò **Teor√≠a**: Conceptos te√≥ricos.\n",
    "- üìö **Lectura**: Material adicional que puedes consultar para profundizar en el tema.\n",
    "- üìä **Ejemplo**: Ejemplo para demostrar y/o reforzar conceptos.\n",
    "- üîó **Enlace**: Recursos externos que puedes visitar para obtener m√°s informaci√≥n.\n",
    "- ‚ùì **Pregunta**: Preguntas disparadas a lo largo del contenido para reflexionar sobre los ejemplos y conceptos tratados.\n",
    "- üíª **C√≥digo**: Indica que la celda de abajo es una celda con c√≥digo y debe ser ejecutada para ver su contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b1a78",
   "metadata": {},
   "source": [
    "## üìò <font color=\"#00b351\">1. Reducci√≥n de dimensionalidad</font>\n",
    "\n",
    "La reducci√≥n de dimensionalidad es una t√©cnica utilizada en el aprendizaje autom√°tico y ciencia de datos **para reducir el n√∫mero de variables bajo consideraci√≥n**, y puede ser dividida en dos tipos: reducci√≥n de caracter√≠sticas y reducci√≥n de muestras.\n",
    "\n",
    "La diferencia entre ambos radica en el enfoque: la reducci√≥n de caracter√≠sticas se centra en seleccionar un subconjunto de las caracter√≠sticas originales, mientras que la reducci√≥n de muestras implica trabajar con un subconjunto de las instancias de datos.\n",
    "\n",
    "En lo que respecta a la reducci√≥n de caracter√≠sticas, las t√©cnicas de reducci√≥n de dimensionalidad nos ayudan a disminuir las dimensiones de los datos a la vez que nos permiten preservar informaci√≥n relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e17430",
   "metadata": {},
   "source": [
    "En este curso aprenderemos sobre las t√©cnicas y m√©todos m√°s comunes para la reducci√≥n de dimensionalidad para el caso de reducci√≥n de caracter√≠sticas.\n",
    "\n",
    "Los objetivos principales de la reducci√≥n de dimensionalidad podr√≠an resumirse en los siguentes puntos:\n",
    "\n",
    "- **Mejora del rendimiento y el entrenamiento del modelo**: Reducir el n√∫mero de dimensiones puede simplificar los modelos, lo que conduce a un entrenamiento m√°s r√°pido y una mejor generalizaci√≥n al minimizar el ruido y las caracter√≠sticas irrelevantes o poco relevantes.\n",
    "- **Reduce los costes computacionales y de almacenamiento**: Un menor n√∫mero de dimensiones requiere menos memoria y tiempo de procesamiento, lo que hace que los algoritmos de aprendizaje autom√°tico sean m√°s eficientes y factibles para grandes conjuntos de datos.\n",
    "- **Mejora la visualizaci√≥n de datos**: Los datos de alta dimensi√≥n son dif√≠ciles de visualizar; la reducci√≥n de dimensi√≥n los transforma en un espacio de menor dimensi√≥n, lo que permite una exploraci√≥n y comprensi√≥n m√°s f√°ciles de los patrones.\n",
    "- **Aborda la maldici√≥n de la dimensionalidad**: A medida que aumenta el n√∫mero de caracter√≠sticas, los datos se vuelven m√°s dispersos, lo que lleva a una degradaci√≥n del rendimiento. La reducci√≥n de dimensi√≥n combate esto creando representaciones m√°s compactas.\n",
    "- **Elimina la informaci√≥n redundante e irrelevante**: Las t√©cnicas de reducci√≥n de dimensi√≥n identifican y separan la informaci√≥n esencial de las caracter√≠sticas redundantes o ruidosas, centr√°ndose en las caracter√≠sticas m√°s destacadas de los datos.\n",
    "- **Simplifica la complejidad de los datos**: Al representar los datos con menos caracter√≠sticas, pero m√°s informativas, se reduce la complejidad general del conjunto de datos, lo que facilita el an√°lisis y la interpretaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f899b",
   "metadata": {},
   "source": [
    "## üìò <font color=\"#00b351\">2. An√°lisis de Componentes Principales o PCA</font>\n",
    "\n",
    "El An√°lisis de Componentes Principales (PCA por sus siglas en ingl√©s) es una t√©cnica de reducci√≥n de dimensionalidad que busca transformar un conjunto de variables posiblemente correlacionadas en un conjunto de variables no correlacionadas, llamadas *Componentes Principales*. Estos componentes principales son combinaciones lineales de las variables originales y se ordenan de tal manera que el primer componente retiene la mayor parte de la variabilidad presente en los datos, el segundo componente retiene la segunda mayor cantidad de variabilidad, y as√≠ sucesivamente.\n",
    "\n",
    "### ¬øC√≥mo funciona el PCA?\n",
    "\n",
    "1. **Estandarizaci√≥n**: Se estandarizan las caracter√≠sticas para que tengan media cero y varianza uno. Esto es importante porque PCA es sensible a la escala de los datos.\n",
    "2. **Covarianza**: Se calcula la matriz de covarianza para entender c√≥mo var√≠an las caracter√≠sticas entre s√≠.\n",
    "3. **Valores y vectores propios**: Se obtienen los valores y vectores propios de la matriz de covarianza. Los vectores propios representan las direcciones de m√°xima varianza, mientras que los valores propios indican la cantidad de varianza en esas direcciones.\n",
    "4. **Selecci√≥n de componentes**: Se seleccionan los primeros k vectores propios (componentes principales) que retienen la mayor parte de la varianza.\n",
    "5. **Proyecci√≥n**: Finalmente, se proyectan los datos originales en el nuevo espacio definido por los componentes principales seleccionados.\n",
    "\n",
    "### Ventajas del PCA\n",
    "\n",
    "- **Reducci√≥n de dimensionalidad**: PCA permite reducir el n√∫mero de caracter√≠sticas manteniendo la mayor parte de la informaci√≥n.\n",
    "- **Eliminaci√≥n de ruido**: Al centrarse en las direcciones de mayor varianza, PCA puede ayudar a eliminar el ruido presente en los datos.\n",
    "- **Visualizaci√≥n**: Facilita la visualizaci√≥n de datos de alta dimensi√≥n al reducirlos a 2 o 3 dimensiones.\n",
    "\n",
    "### Desventajas del PCA\n",
    "\n",
    "- **P√©rdida de informaci√≥n**: Aunque PCA busca preservar la varianza, siempre hay una p√©rdida de informaci√≥n al reducir dimensiones.\n",
    "- **Interpretabilidad**: Los componentes principales son combinaciones lineales de las caracter√≠sticas originales, lo que puede dificultar la interpretaci√≥n de los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaec3da",
   "metadata": {},
   "source": [
    "üíª Importando librer√≠as üíª\n",
    "\n",
    "Primero, vamos a importar todas las librer√≠as necesarias. Por favor, ejecuta la celda de c√≥digo de abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db87e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **** C√ìDIGO PYTHON ****\n",
    "\n",
    "##Clonamos el repositorio para poder usar las funciones\n",
    "## Esperar unos segundos hasta ver un 100% de descarga\n",
    "# !git clone https://github.com/lucasbaldezzari/cdan.git\n",
    "\n",
    "##importamos las funciones a usar\n",
    "from funciones.dimension_reduction import *\n",
    "# from funciones.utils import * ##importo funciones a usar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56a2f8",
   "metadata": {},
   "source": [
    "#### üìò <font color=\"#00b351\">2.1 Intepretando proyecciones en PCA</font>\n",
    "\n",
    "Lo que debemos entender es que PCA busca *ejes*, tambi√©n llamados *componentes* que captan la **varianza** en los datos. Estos ejes son direcciones en el espacio de caracter√≠sticas que maximizan la variabilidad de los datos proyectados en ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d68b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPCAExamples(figsize=(15,5),n_points=100,std1=1.2,std2=0.2,scalepc1=0.5,scalepc2=1.8,show_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61ca86",
   "metadata": {},
   "source": [
    "üìò\n",
    "\n",
    "En el ejemplo anterior se puede observar que pareciera que hay una relaci√≥n entre $x_1$ y $x_2$. Adem√°s, se puede observar el antes y el despu√©s de aplicar PCA. Debemos entender que que la transformaci√≥n que hace PCA no modifica los datos originales, sino que crea una nueva representaci√≥n de los datos en un nuevo espacio de caracter√≠sticas. A su vez, cada componente principal es una combinaci√≥n lineal de las caracter√≠sticas originales lo cual tiene ventajas y desventajas.\n",
    "\n",
    "Como ventaja podemos decir que al combinar caracter√≠sticas, PCA puede capturar patrones subyacentes en los datos que no son evidentes en las caracter√≠sticas originales. Sin embargo, como desventaja, esta combinaci√≥n lineal puede dificultar la interpretaci√≥n de los componentes principales, ya que no corresponden directamente a las caracter√≠sticas originales.\n",
    "\n",
    "Debido a que cada componente principal es una combinaci√≥n lineal de las caracter√≠sticas originales, podemos tomar menos componentes principales que caracter√≠sticas originales, lo cual es una ventaja en t√©rminos de reducci√≥n de dimensionalidad.\n",
    "\n",
    "üìò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbd2a3",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">2.2 Aplicando PCA para visualizar datos</font>**\n",
    "\n",
    "En este primer ejemplo utilizaremos el set de datos conocido como MNIST.\n",
    "\n",
    "El MNIST es un conjunto de datos que contiene im√°genes de d√≠gitos manuscritos (del 0 al 9) y es ampliamente utilizado para entrenar y evaluar modelos de aprendizaje autom√°tico.\n",
    "\n",
    "Los datos de MNIST consisten en $60000$ im√°genes de entrenamiento y $10000$ im√°genes de prueba, cada una de las cuales es una imagen en escala de grises de $28\\times28$ p√≠xeles. Cada imagen est√° etiquetada con el d√≠gito correspondiente que representa. En este ejemplo, tomaremos algunos n√∫meros aleatoriamente y aplicaremos PCA para entender c√≥mo se pueden reducir las dimensiones de estas im√°genes mientras se preserva la informaci√≥n relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73f7aa",
   "metadata": {},
   "source": [
    "üíª Graficando algunos n√∫meros üíª\n",
    "\n",
    "Vamos a graficar algunos n√∫meros aleatorios del dataset MNIST. Por favor, ejecuta la celda de c√≥digo de abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSomeNumbers(cmap=\"gray\",figsize=(15,5), seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08376116",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øCu√°l es la dimensi√≥n del espacio de caracter√≠sticas de cada imagen (n√∫mero) dentro de MNIST?\n",
    "\n",
    "---‚ùì--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dbb7e",
   "metadata": {},
   "source": [
    "üíª Zoom sobre un n√∫mero üíª\n",
    "\n",
    "**SIEMPRE** es importante saber c√≥mo est√°n formados nuestros datos. En este caso, cada imagen es una matriz de $28\\times28$ p√≠xeles, lo que significa que cada imagen tiene $784$ caracter√≠sticas (p√≠xeles) si la \"aplanamos\" en un vector.\n",
    "\n",
    "En cada p√≠xel de la imagen, el valor que toma representa la intensidad del color en escala de grises. Un $0$ representa el color negro, mientras que un $255$ representa el color blanco. Los valores intermedios representan diferentes tonos entre el blanco y el negro.\n",
    "\n",
    "Por favor, ejecuta la celda de c√≥digo de abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf84a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_start=10\n",
    "r_end=15\n",
    "makeZoom(r_start=r_start, r_end=r_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df45fd6",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øC√≥mo se te ocurre que podr√≠amos visualizar la distribuci√≥n de cada n√∫mero en un espacio de caracter√≠sticas de dos dimensiones para ver como se distribuyen los n√∫meros?\n",
    "\n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57d966",
   "metadata": {},
   "source": [
    "üíª Promediando las intensidades de los p√≠xeles üíª\n",
    "\n",
    "Una posible soluci√≥n a la respuesta anterior ser√≠a tomar algunas de las caracter√≠sticas de cada imagen y graficar un scatter plot, por ejemplo, podr√≠amos pensar en las intensidades de los p√≠xeles para representar cada n√∫mero en un espacio de dos dimensiones. Sin embargo, tenemos $256$ valores diferentes de intensidad, ¬øcu√°l ser√≠a la mejor opci√≥n?, tal vez podr√≠amos tomar el promedio de las intensidades de p√≠xeles de las filas y de las columnas y graficar estas proyecciones en 2D.\n",
    "\n",
    "Por favor, ejecuta la celda de abajo para ver c√≥mo se \"ven\" los n√∫meros en 2D usando los promedios de intensidades de filas vs columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMNIST2DUsingMeans()\n",
    "# plotMNIST2DUsingMeans_px()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c4086",
   "metadata": {},
   "source": [
    "üòñüò©üò¢üíÄ\n",
    "\n",
    "Podemos ver que la representaci√≥n de la informaci√≥n usando los promedios de intensidad de filas y columnas **NO** nos permite diferenciar donde se encuentran los diferentes d√≠gitos en el espacio de caracter√≠sticas de 2D. Es **importante** saber qu√© usando estas caracter√≠sticas no podremos clasificar los n√∫meros correctamente.\n",
    "\n",
    "üòñüò©üò¢üíÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01cf701",
   "metadata": {},
   "source": [
    "üíª Usando PCA para generar gr√°fico en 2D üíª\n",
    "\n",
    "Vamos ahora a usar PCA.\n",
    "\n",
    "En este caso lo que haremos es aplicar PCA a los datos originales y usar las primeras dos componentes principales para graficar los datos en un espacio de dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMINST2dUsingPCA(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96733c0b",
   "metadata": {},
   "source": [
    "üíª Scatterplot interactivo üíª\n",
    "\n",
    "Ejecutando la celda de abajo podr√°s ver un scatterplot interactivo donde cada punto representa una imagen del dataset MNIST proyectada en las dos primeras componentes principales obtenidas mediante PCA. Podes hacer zoom y desplazarte por el gr√°fico para explorar c√≥mo se distribuyen los diferentes d√≠gitos en este espacio reducido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMNIST2DUsingPCA_px(point_size=8,opacity=0.5,show_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a0b04",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. En base al gr√°fico anterior, ¬øqu√© n√∫meros son m√°s f√°ciles de separar y cu√°les son m√°s dif√≠ciles de separar? ¬øPor qu√© crees que es as√≠?\n",
    "2. ¬øCrees que podr√≠amos clasificar con cierto √©xito los n√∫meros usando solo estas dos componentes principales? ¬øPor qu√©? ¬øY si us√°ramos m√°s componentes?\n",
    "\n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604b37c",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">2.3 Aplicando PCA para reducir dimensiones y preservar informaci√≥n</font>**\n",
    "\n",
    "La idea de este ejemplo es poder entender c√≥mo PCA preserva informaci√≥n relevante en las componentes principales, a su vez que nos permite reducir la dimensionalidad de los datos.\n",
    "\n",
    "El algoritmo de PCA *\"se entrena\"*, es decir, obtiene sus componentes principales, a partir de los datos originales. Hemos visto que podemos proyectar los datos en el espacio de las componentes, sin embargo, <font color=\"#002ab3\">con PCA podemos **reconstruir** el espacio original a partir de menos componentes</font>. Es importante recordar que <font color=\"#f30010\">las componentes principales se ordenan seg√∫n la cantidad de varianza que explican</font>, a su vez, mayor varianza en el contexto de PCA significa mayor informaci√≥n. Por lo tanto, si tomamos las primeras $K$ componentes principales, estaremos reteniendo la mayor parte de la informaci√≥n presente en los datos originales, y a partir de estas componentes, podr√≠amos reconstruir el espacio de datos origninal, pero eso s√≠, con cierta p√©rdida de informaci√≥n.\n",
    "\n",
    "Recordemos que la cantidad de componentes principales que se pueden obtener es $N$, donde $N$ es la dimensionalidad original de los datos. Ahora bien, nosotros podemos usar una $K$ cantidad de componentes principales, donde $K \\leq N$ para reconstruir los datos originales, pero usando solo $K$ componentes principales. De esta manera, si $K < N$, estaremos reduciendo la dimensionalidad de los datos o bien haciendo una compresi√≥n de los datos.\n",
    "\n",
    "Veamos esto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45deebf",
   "metadata": {},
   "source": [
    "üíª \n",
    "\n",
    "Para observar el impacto que tiene de usar m√°s o menos componentes principales, vamos a utilizar diferente cantidades de componentes principales para reconstruir las im√°genes originales. De esta manera podremos ver c√≥mo la cantidad de componentes principales afecta la calidad de la reconstrucci√≥n.\n",
    "\n",
    "Por favor, modifica el valor de `num_components` en la celda de c√≥digo de abajo y ejecuta la celda para ver c√≥mo cambia la calidad de la reconstrucci√≥n.\n",
    "\n",
    "NOTA: Recordar que las im√°genes originales tienen $784$ caracter√≠sticas (p√≠xeles), es decir, el espacio original es de $784$ dimensiones.\n",
    "\n",
    "üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = None\n",
    "digit_to_show = 5\n",
    "plotDigitReconstruction(num_components=num_components, digit=digit_to_show, figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c0096",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øPor qu√© cuando usamos una cantidad de componentes principales igual a $784$ la imagen reconstruida es id√©ntica a la original?\n",
    "2. ¬øQu√© sucede con la calidad de la reconstrucci√≥n a medida que disminu√≠mos la cantidad de componentes principales usadas para la reconstrucci√≥n? ¬øPor qu√©?\n",
    "3. ¬øEs posible discernir qu√© n√∫mero es en la imagen reconstruida cuando usamos solo $5$ componentes principales? ¬øY con 1?\n",
    "4. Seleccionando una componente, ¬øel n√∫mero $0$ es posible de reconocer? ¬øy el 9?\n",
    "\n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c294778",
   "metadata": {},
   "source": [
    "#### **<font color=\"#d60217\"> IMPORTANTE</font>**\n",
    "\n",
    "Con el ejemplo anterior hemos visto que PCA nos permite reducir la dimensionalidad de los datos mientras preservamos la mayor parte de la informaci√≥n relevante. Sin embargo, es importante tener en cuenta que la reducci√≥n de dimensionalidad siempre implica una p√©rdida de informaci√≥n, y la calidad de la reconstrucci√≥n depender√° de la cantidad de componentes principales utilizadas. Cuando reconstruimos los datos a partir de $K$ con $K \\leq N$ y $N$ siendo la cantidad de caracter√≠sticas originales, lo que se hace es **arpoximar** los datos a partir de las $K$ componentes principales seleccionadas, y esta aproximaci√≥n no ser√° perfecta, es decir, habr√° p√©rdida de informaci√≥n, a menos que usemos todas las componentes principales, es decir, $K = N$.\n",
    "\n",
    "**¬øQu√© usamos en problemas de clasificaci√≥n?**\n",
    "\n",
    "Por otro lado, en problemas de clasificaci√≥n, en general, no se usa el espacio reconstruido, sino que se usan las componentes principales como nuevas caracter√≠sticas para entrenar un modelo de clasificaci√≥n. Esto se debe a que las componentes principales capturan la mayor parte de la variabilidad en los datos, lo que puede mejorar el rendimiento del modelo al reducir el ruido y las caracter√≠sticas irrelevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90621f53",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">2.4 Determinando la cantidad √≥ptima de componentes principales</font>**\n",
    "\n",
    "En este ejemplo usaremos la varianza acumulada para determinar la cantidad √≥ptima de componentes principales a usar.\n",
    "\n",
    "Recordemos que cada componente principal explica una cierta cantidad de varianza en los datos. La varianza acumulada es la suma de las varianzas explicadas por cada componente principal hasta un cierto punto.\n",
    "\n",
    "A medida que se agregan m√°s componentes principales, la varianza acumulada aumenta, ya que cada componente adicional contribuye con m√°s varianza explicada. Sin embargo, la tasa de aumento de la varianza acumulada disminuye a medida que se agregan m√°s componentes, lo que indica que las primeras componentes principales capturan la mayor parte de la varianza en los datos.\n",
    "\n",
    "Ahora bien, en general se considera que una cantidad √≥ptima de componentes principales es aquella que logra explicar un porcentaje significativo de la varianza total, como el 80% al 95%, mientras se mantiene un n√∫mero manejable de componentes para evitar la sobrecarga computacional y la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d2c9b",
   "metadata": {},
   "source": [
    "üíª Ejecuta la celda de abajo para ver el gr√°fico de varianza acumulada y determinar la cantidad √≥ptima de componentes principales a usar. üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = 100\n",
    "varianceExplainedPlot(max_components=max_components, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb377f",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øQu√© informaci√≥n podes sacar del gr√°fico de varianza acumulada?\n",
    "  \n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9cd0c",
   "metadata": {},
   "source": [
    "üíª\n",
    "\n",
    "Analizando la gr√°fica anterior eleg√≠ un n√∫mero de componentes y usalo en la celda de abajo para reconstruir la imagen del d√≠gito.\n",
    "Reemplaza el valor *None* por el n√∫mero de componentes que creaste adecuado.\n",
    "\n",
    "üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3082996",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = None\n",
    "digit_to_show = 5\n",
    "plotDigitReconstruction(num_components=num_components, digit=digit_to_show, figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287dd8e",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øCu√°nta varianza acumulada has elegido?\n",
    "2. ¬øCrees que con la cantidad de componentes que elegiste se podr√≠a clasificar los n√∫meros con un buen desempe√±o? ¬øPor qu√©?\n",
    "3. ¬øQu√© conclusiones podes sacar sobre la relaci√≥n entre la cantidad de componentes principales y la calidad de la reconstrucci√≥n de las im√°genes?\n",
    "   \n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036412a",
   "metadata": {},
   "source": [
    "## üìò <font color=\"#00b351\">3. An√°lisis de Discrminante Lineal</font>\n",
    "\n",
    "LDA es un m√©todo de reducci√≥n de dimensionalidad del tipo supervisado, el objetivo es maximizar la separabilidad entre clases.\n",
    "\n",
    "PCA busca las direcciones que capturan la mayor varianza de nuestros datos (mayor informaci√≥n), mientras que LDA busca direcciones que separen mejor los grupos o clases conocidas.\n",
    "\n",
    "Ejemplo: si tenemos datos con etiquetas (tipos de clientes, productos, o fraude/no fraude), <font color=\"#00b351\">**LDA nos ayuda a encontrar una proyecci√≥n que separe esos grupos de la forma m√°s clara posible, adem√°s de que nos permite reducir las dimensiones de nuestros datos**</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01411049",
   "metadata": {},
   "source": [
    "#### ¬øC√≥mo hace LDA para aumentar la separabilidad entre clases?\n",
    "\n",
    "Se consideran los siguientes criterios de manera simultanea:\n",
    "\n",
    "- Maximizando la distancia entre dos clases: Esta distancia refiere a la diferencia entre las medias de las clases, es decir, ùúá_1 y ùúá_2. ¬øY si tenemos m√°s de dos clases?\n",
    "- Minimizando la variaci√≥n o dispersi√≥n.\n",
    "\n",
    "Supongamos que tenemos un set de datos con dos clases. LDA busca aumentar al m√°ximo la separaci√≥n entre las medias de las clases mientras minimiza la varianza dentro de cada clase. Esto se puede expresar matem√°ticamente como la maximizaci√≥n de la siguiente raz√≥n:\n",
    "\n",
    "$$ \\frac{(\\mu_1 - \\mu_2)^2}{s_1^2 + s_2^2} $$\n",
    "\n",
    "Donde $\\mu_1$ y $\\mu_2$ son las medias de cada clase, respectivamente y $s_1^2$ y $s_2^2$ son las varianzas de cada clase, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173d87f",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">3.1 PCA vs LDA para analizar y separar datos</font>**\n",
    "\n",
    "Para explicar LDA, usaremos un dataset con precios de casas. Este dataset contiene informaci√≥n sobre diferentes caracter√≠sticas de las casas, como ser la edad de la casa, qu√© tan cerca est√° de alg√∫n medio de transporte p√∫blico, tiendas cercanas, entre otras caracter√≠sticas, junto con el precio por unidad de √°rea de la casa.\n",
    "\n",
    "A este set de datos se le ha creado una nueva columna llamada *Categoria* que clasifica los precios de las casas en tres categor√≠as: *Econ√≥mica*, *Rango-Medio* y *Cara*. Esta categorizaci√≥n nos permitir√° analizar los datos y adem√°s aplicar LDA para encontrar una proyecci√≥n que maximice la separaci√≥n entre estas tres clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27ffb2",
   "metadata": {},
   "source": [
    "üíª Ejecuta la celda de abajo para analizar c√≥mo se distribuyen los datos en el **espacio original** üíª "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotRealEstatePCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dfa5f",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øQu√© caracter√≠sticas se han utilizado para el gr√°fico anterior?\n",
    "2. ¬øCrees que las categor√≠as ser√≠an f√°ciles de clasificar usando estas dos caracter√≠sticas? ¬øPor qu√©?\n",
    "   \n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead5d4d",
   "metadata": {},
   "source": [
    "#### üìä **<font color=\"#d6b302\">3.2 Aplicando PCA para analizar los datos </font>**\n",
    "\n",
    "Primero vamos a aplicar PCA para proyectar los datos en dos dimensiones y ver c√≥mo se distribuyen las diferentes categor√≠as de precios de las casas usando las dos primeras componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "houserWithPCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11dd1e",
   "metadata": {},
   "source": [
    "Si prestamos atenci√≥n al *RESUMEN DE PCA* podemos ver que las dos primeras componentes principales explican aproximadamente el $70\\%$ de la varianza total de los datos. Esto significa que al proyectar los datos en estas dos dimensiones, estamos reteniendo una cantidad significativa de la informaci√≥n original.\n",
    "\n",
    "Ahora bien, podemos ver que tenemos una gran dispersi√≥n de puntos en el plano proyectado. Dado que PCA s√≥lo considera la varianza de las caracter√≠sticas, y no la de las clases o target, no es de extra√±ar que las diferentes categor√≠as no est√©n bien separadas en este espacio proyectado y por lo tanto, las tres categor√≠as est√°n mezcladas entre s√≠.\n",
    "\n",
    "Veamos si podemos mejorar la separabilidad entre las clases usando LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa9ac3",
   "metadata": {},
   "source": [
    "#### üìä **<font color=\"#d6b302\">3.3 Aplicando LDA </font>**\n",
    "\n",
    "Ahora vamos a aplicar LDA para proyectar los datos en dos dimensiones y ver c√≥mo se distribuyen las diferentes categor√≠as de precios de las casas usando las primeras dos componentes de LDA.\n",
    "\n",
    "Usaremos las mismas caracter√≠sticas que usamos para PCA y entrenaremos un LDA para encontrar las dos mejores direcciones que maximicen la separabilidad entre las tres categor√≠as de precios de las casas. Graficaremos las proyecciones en 2D de LD1 vs LD2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45151f6",
   "metadata": {},
   "source": [
    "üíª Por favor, ejecuta la celda de abajo. üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üíª Implementaci√≥n de LDA\n",
    "houserWithLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeabbf7",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øQu√© podes decir de la separaci√≥n entre las categor√≠as usando LDA?\n",
    "2. ¬øQu√© podes decir acerca de la varianza dentro de cada categor√≠a si la comparamos con PCA?\n",
    "   \n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdf8b9",
   "metadata": {},
   "source": [
    "## üìä **<font color=\"#d6b302\">4. PCA vs LDA para clasificar</font>**\n",
    "\n",
    "En este ejemplo vamos a comparar PCA y LDA en t√©rminos de su capacidad para separar y clasificar las diferentes categor√≠as de precios de las casas.\n",
    "\n",
    "<font color=\"#aa1602\">**NOTA**: En la √∫ltima clase del curso profundizaremos en el proceso de preparaci√≥n de los datos, entrenamiento y evaluaci√≥n de modelos de clasificaci√≥n.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9280008",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">4.1 Algunos clasificadores con PCA</font>**\n",
    "\n",
    "En este primer ejemplo vamos a usar un clasificador SVM para clasificar las casas en las tres categor√≠as mencionadas anteriormente, usando PCA para reducir la dimensionalidad de los datos a dos dimensiones y as√≠ poder visualizar los resultados. Usaremos los dos primeros componentes principales obtenidas mediante PCA como nuevas caracter√≠sticas para entrenar el modelo SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d3a0",
   "metadata": {},
   "source": [
    "üíª Ejecuta la celda de abajo para entrenar y evaluar los resultados de la clasificaci√≥n usando **PCA y SVM** üíª "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e10b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiedUsingPCA(clasificador='SVM', num_comps=2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af111260",
   "metadata": {},
   "source": [
    "üíª Ejecuta la celda de abajo para entrenar y evaluar los resultados de la clasificaci√≥n usando **PCA y Decision Tree** üíª "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiedUsingPCA(clasificador='DecisionTree', num_comps=2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86da409",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. En base a los reportes de clasificaci√≥n y las matrices de confusi√≥n, ¬øqu√© m√©todo (PCA+SVM o PCA+Decision Tree) crees que funcion√≥ mejor para clasificar las casas en las tres categor√≠as? ¬øPor qu√©?\n",
    "2. ¬øHay alguna categor√≠a que sea m√°s dif√≠cil de clasificar? ¬øCu√°l es?\n",
    "   \n",
    "---‚ùì---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b72fa",
   "metadata": {},
   "source": [
    "###  üìò 4.2 Reporte de clasificaci√≥n\n",
    "\n",
    "\n",
    "#### M√©tricas de evaluaci√≥n\n",
    "\n",
    "Es importante saber interpretar los reportes de clasificaci√≥n y las matrices de confusi√≥n para evaluar el desempe√±o de los modelos de clasificaci√≥n.\n",
    "\n",
    "En primer lugar, debemos recordar que estamos trabajando en un problema de clasificaci√≥n multiclase, en este caso, las clases son *Econ√≥mica*, *Rango-Medio* y *Cara*. En este contexto, el reporte calcula **por cada una de las clases* las siguientes m√©tricas:\n",
    "\n",
    "- **Precisi√≥n (Precision)**: La precisi√≥n es la proporci√≥n de verdaderos positivos entre el total de predicciones positivas realizadas por el modelo. En otras palabras, mide cu√°ntas de las predicciones positivas fueron correctas. Una alta precisi√≥n indica que el modelo tiene pocos falsos positivos. Se calcula como:\n",
    "\n",
    "  $$ \\text{Precisi√≥n} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos} + \\text{Falsos Positivos}} $$\n",
    "\n",
    "Esto se puede leer como, \"de todo lo que el modelo predijo como clase A, ¬øqu√© fracci√≥n era realmente clase A?\".\n",
    "\n",
    "- **Recall**: El recall, tambi√©n conocido como *\"tasa de verdaderos positivos\"* o *\"sensibilidad\"* es la proporci√≥n de verdaderos positivos entre el total de casos positivos reales. Mide cu√°ntos de los casos positivos reales fueron correctamente identificados por el modelo. Una alta sensibilidad indica que el modelo tiene pocos falsos negativos. Se calcula como:\n",
    "\n",
    "  $$ \\text{Recall} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos} + \\text{Falsos Negativos}} $$\n",
    "\n",
    "Esto se puede leer como, \"de todo lo que realmente era clase A, ¬øqu√© fracci√≥n fue correctamente identificada?\".\n",
    "\n",
    "- Columna *support*: Indica la cantidad de instancias reales de cada clase en el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076062d4",
   "metadata": {},
   "source": [
    "#### Una m√©trica por clase\n",
    "\n",
    "Podemos ver que tenemos tres filas para cada una de las m√©tricas mencionadas anteriormente, es decir, una fila por cada clase. A modo de ejemplo, en el caso de SVM + PCA tenemos que la clase *Economica* tiene una precisi√≥n de $0.69$ y un recall de $0.51$, es decir, de todas las predicciones que el modelo hizo como *Econ√≥mica*, el $69\\%$ fueron correctas, y de todas las casas que realmente eran *Econ√≥micas*, el $51\\%$ fueron correctamente identificadas por el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7051fe",
   "metadata": {},
   "source": [
    "#### M√©tricas globales\n",
    "\n",
    "Finalmente tenemos las m√©tricas globales, las cuales son:\n",
    "\n",
    "- **Accuracy**: La proporci√≥n total de aciertos del modelo, que es la proporci√≥n de predicciones correctas (tanto verdaderos positivos como verdaderos negativos) entre el total de predicciones realizadas. Se calcula como:\n",
    "\n",
    "  $$ \\text{Accuracy} = \\frac{\\text{\\#Predicciones Correctas}}{\\text{\\#Total de muestras}} $$\n",
    "\n",
    "- **Macro avg**: Es el promedio no ponderado de las m√©tricas por clase. Se trata a todas las clases por igual, aunque est√©n desbalanceadas.\n",
    "- **Weighted avg**: Es el promedio ponderado de las m√©tricas por clase, teniendo en cuenta el soporte (n√∫mero de instancias) de cada clase. Esto es √∫til cuando las clases est√°n desbalanceadas, ya que da m√°s peso a las clases con m√°s instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01633ba2",
   "metadata": {},
   "source": [
    "#### Matriz de confusi√≥n\n",
    "\n",
    "La matriz de confusi√≥n es una herramienta que nos permite visualizar el desempe√±o de un modelo de clasificaci√≥n. En una matriz de confusi√≥n se comparan las predicciones del modelo con las etiquetas reales de las clases y se puede observar cu√°ntas veces el modelo predijo correctamente cada clase y cu√°ntas veces cometi√≥ errores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64441778",
   "metadata": {},
   "source": [
    "#### ¬øQu√© podemos inferir con todo esto? üòÄ\n",
    "\n",
    "- Qu√© clase los clasificadores confunden m√°s. Por ejemplo, baja precisi√≥n implican muchos FP; bajo recall implica muchos FN.\n",
    "- El *Weighted avg* es una m√©trica √∫til cuando las clases est√°n desbalanceadas, ya que da m√°s peso a las clases con m√°s instancias.\n",
    "- Seg√∫n el problema, deberemos darle m√°s importancia a reducir los FN o los FP, en el primer caso deberemos priorizar el recall, y en el segundo caso la precisi√≥n üòé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701734a5",
   "metadata": {},
   "source": [
    "### üìä **<font color=\"#d6b302\">4.1 Algunos clasificadores con LDA</font>**\n",
    "\n",
    "Ahora realizaremos el mismo procedimiento que hicimos con PCA, pero esta vez usando LDA para reducir la dimensionalidad de los datos a dos dimensiones y as√≠ poder visualizar los resultados. Usaremos las dos primeras componentes de LDA como nuevas caracter√≠sticas para entrenar el modelo SVM y Decision Tree.\n",
    "\n",
    "Ejecuta la celda de abajo para entrenar y evaluar los resultados de la clasificaci√≥n usando **LDA y SVM** üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiedUsingLDA(clasificador='SVM', num_comps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiedUsingLDA(clasificador='DecisionTree', num_comps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2c857",
   "metadata": {},
   "source": [
    "üíª Comparando matrices de confusi√≥n para SVM üíª\n",
    "\n",
    "Ejecuta las celdas debajo para comparar las matrices de confusi√≥n obtenidas usando PCA y LDA con el clasificador SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad94fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compararCMs(clasificador=\"SVM\")\n",
    "compararCMs(clasificador=\"DecisionTree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39228f6d",
   "metadata": {},
   "source": [
    "---‚ùì--- \n",
    "\n",
    "1. ¬øCu√°l combinaci√≥n de t√©cnicas funcion√≥ mejor para clasificar las casas en las tres categor√≠as? ¬øPor qu√©?\n",
    "2. ¬øHay alguna categor√≠a que sea m√°s dif√≠cil de clasificar? ¬øCu√°l es?\n",
    "3. Comparando los resultados obtenidos con PCA y LDA, ¬øcu√°l m√©todo crees que funcion√≥ mejor para clasificar las casas en las tres categor√≠as? ¬øPodr√≠as explicar por qu√©?\n",
    "\n",
    "---‚ùì--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219217f7",
   "metadata": {},
   "source": [
    "## üìò <font color=\"#00b351\">5. Conclusiones</font>\n",
    "\n",
    "Hemos visto que con PCA podemos reducir la dimensionalidad de los datos mientras preservamos una cantidad significativa de la varianza total. PCA es sumamente √∫til cuando queremos entender la estructura subyacente de los datos y reducir el ruido, o bien cuando queremos *comprimir* los datos para hacerlos m√°s manejables. Cuando decimos *ciomprimir* nos referimos a que podemos representar los datos originales usando menos dimensiones, lo que puede ser beneficioso para el almacenamiento y la eficiencia computacional.\n",
    "\n",
    "Sin embargo, PCA no considera las etiquetas de clase, lo que puede resultar en una mezcla de categor√≠as en el espacio proyectado. Esto lo pudimos ver con el set de datos de precios de casas, donde las categor√≠as estaban mezcladas entre s√≠ (ver secci√≥n 3.2).\n",
    "\n",
    "Por otro lado, si observamos la secci√≥n 3.3 podemos notar una gran diferencia en la distribuci√≥n de las categor√≠as entre PCA y LDA. Mientras que en PCA las categor√≠as est√°n mezcladas, en LDA observamos una separaci√≥n m√°s clara entre las categor√≠as, lo que indica que LDA ha logrado encontrar direcciones en el espacio de caracter√≠sticas que maximizan la separabilidad entre las clases. Por otro lado, la varianza dentro de cada categor√≠a es menor en LDA en comparaci√≥n con PCA, lo que sugiere que los puntos dentro de cada clase est√°n m√°s agrupados y son m√°s similares entre s√≠."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
